apiVersion: logging.vector.io/v1
kind: LogOutput
metadata:
  name: multi-output
  namespace: logging-system
spec:
  sinks:
    # 1. 所有日志存储到 Elasticsearch
    elasticsearch_all:
      type: elasticsearch
      inputs: ["*"]
      config:
        endpoint: "http://elasticsearch-master:9200"
        index: "logs-%Y-%m-%d"
        bulk:
          action: "create"
          batch_size: 10000000  # ~10MB
          timeout_secs: 30
    
    # 2. 错误日志发送到 Kafka 告警主题
    kafka_errors:
      type: kafka
      # 引用 LogFlow 中定义的过滤转换器
      inputs: ["filter_errors"]
      config:
        bootstrap_servers: "kafka-alerting:9092"
        topic: "error-alerts"
        encoding:
          codec: json
        compression: "gzip"
        key_field: ".kubernetes.pod_name"
    
    # 3. 指标数据发送到 Prometheus
    prometheus_metrics:
      type: prometheus_exporter
      inputs: ["host_metrics"]
      config:
        address: "0.0.0.0:9090"
    
    # 4. 所有日志长期归档到 S3
    s3_archive:
      type: aws_s3
      inputs: ["*"]
      config:
        bucket: "logs-archive"
        region: "us-west-2"
        key_prefix: "k8s-logs/%Y/%m/%d/{{ kubernetes.namespace }}/{{ kubernetes.pod_name }}"
        compression: gzip
        encoding:
          codec: json
        # 使用 AWS 环境变量或实例角色获取凭证
        auth:
          assume_role: null
          # access_key_id 和 secret_access_key 通常通过环境变量提供
        
        # S3 相关的高级设置
        batch:
          max_bytes: 10485760  # 10MB
          timeout_secs: 300
        
        # 缓冲区设置，确保在网络问题时不丢失数据
        buffer:
          type: "disk"
          max_size: 1073741824  # 1GB
          when_full: "block" 